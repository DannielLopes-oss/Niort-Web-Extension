# backend/brain.py
import os
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain_core.documents import Document

# 1. Configura√ß√£o do Modelo de Embeddings (Transforma texto em n√∫meros)
# Este modelo √© leve, r√°pido e vai rodar direto na sua CPU
EMBEDDING_MODEL = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 2. Configura√ß√£o do Banco Vetorial (ChromaDB)
# O hist√≥rico ficar√° salvo localmente nesta pasta para n√£o perder quando reiniciar
PERSIST_DIRECTORY = "./niort_db"

vector_store = Chroma(
    collection_name="niort_history",
    embedding_function=EMBEDDING_MODEL,
    persist_directory=PERSIST_DIRECTORY
)

# 3. Fun√ß√£o para Salvar o Hist√≥rico na Mem√≥ria
def learn_from_history(history_items):
    """
    Recebe a lista de hist√≥rico, transforma em Documentos e salva no ChromaDB.
    """
    documents = []
    for item in history_items:
        # Criamos um "Documento" com o conte√∫do (t√≠tulo + url) e metadados
        content = f"Title: {item.title}\nURL: {item.url}"
        doc = Document(
            page_content=content,
            metadata={"url": item.url, "visit_time": item.visit_time}
        )
        documents.append(doc)

    # Adiciona ao banco (isso cria os vetores automaticamente)
    if documents:
        vector_store.add_documents(documents)
        print(f"üß† Niort aprendeu {len(documents)} novos itens!")
    return True

# 4. Fun√ß√£o para Perguntar ao Niort (RAG)
def ask_niort(question: str):
    """
    1. Busca no hist√≥rico algo relacionado √† pergunta.
    2. Envia o contexto + pergunta para o Ollama.
    """
    print(f"ü§î Pensando sobre: {question}")
    
    # A. Busca os 3 itens mais relevantes no hist√≥rico
    results = vector_store.similarity_search(question, k=3)
    
    context_text = "\n\n".join([doc.page_content for doc in results])
    sources = [doc.metadata.get('url', 'URL n√£o encontrada') for doc in results]

    # B. Prepara o prompt para o Ollama
    prompt = f"""
    Voc√™ √© o Niort Bot, um assistente pessoal inteligente.
    
    Use o seguinte contexto do hist√≥rico de navega√ß√£o do usu√°rio para responder √† pergunta.
    Se a resposta n√£o estiver no contexto, use seu conhecimento geral, mas avise que n√£o encontrou no hist√≥rico.
    
    CONTEXTO DO USU√ÅRIO (Hist√≥rico recente):
    {context_text}
    
    PERGUNTA DO USU√ÅRIO:
    {question}
    
    Responda em portugu√™s, de forma direta e amig√°vel.
    """

    # C. Chama o Ollama (Llama 3 rodando localmente)
    try:
        llm = OllamaLLM(model="gemma2:2b") 
        response = llm.invoke(prompt)
    except Exception as e:
        print(f"Erro ao chamar Ollama: {e}")
        return {
            "reply": "Desculpe, n√£o consegui conectar ao Ollama. Verifique se ele est√° rodando com 'ollama run llama3'.",
            "related_links": []
        }

    return {
        "reply": response,
        "related_links": sources
    }
    
def generate_recommendation():
    """V√™ o hist√≥rico recente e gera uma dica sutil de leitura/v√≠deo."""
    print("üîç Procurando ideias para recomendar...")
    
    # Pega os 5 √∫ltimos itens salvos no banco para entender o contexto atual
    try:
        results = vector_store.similarity_search("o que o usu√°rio mais pesquisa ou estuda?", k=5)
        if not results:
            return None
            
        context_text = "\n".join([doc.page_content for doc in results])

        prompt = f"""
        Voc√™ √© o Niort Bot. Baseado nestes sites que o usu√°rio visitou recentemente:
        {context_text}

        Escreva UMA frase muito curta (m√°ximo 12 palavras) sugerindo um assunto que ele pode gostar de pesquisar agora.
        Exemplo: "Que tal ver um artigo sobre FastAPI?" ou "Encontrei v√≠deos novos sobre Python."
        Seja sutil e amig√°vel. N√ÉO use aspas na sua resposta.
        """
        
        llm = OllamaLLM(model="gemma2:2b")
        response = llm.invoke(prompt)
        return response.strip()
    except Exception as e:
        print(f"Erro ao gerar recomenda√ß√£o: {e}")
        return None